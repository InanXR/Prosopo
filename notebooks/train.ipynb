{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosopo Training Notebook\n",
    "\n",
    "Train a face embedding model from scratch using ArcFace loss.\n",
    "\n",
    "**Target:** 99%+ accuracy on LFW benchmark\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Mount Drive (checkpoint survival)\n",
    "2. Download CASIA-WebFace from Kaggle (.rec format)\n",
    "3. Unpack RecordIO ‚Üí raw images\n",
    "4. Align faces with MTCNN ‚Üí 112√ó112\n",
    "5. Train ResNet-50 + ArcFace\n",
    "6. Evaluate on LFW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Mount Drive\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL:** Mount Drive FIRST to ensure checkpoints survive session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/prosopo/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "print('‚úÖ Drive mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q albumentations facenet-pytorch scikit-image\n",
    "!pip install -q tqdm scikit-learn opencv-python\n",
    "!pip install -q mxnet  # For RecordIO unpacking\n",
    "!pip install -q kaggle\n",
    "\n",
    "print('‚úÖ Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Prosopo repo and verify structure\n",
    "!git clone https://github.com/InanXR/Prosopo.git /content/prosopo\n",
    "\n",
    "# Verify the package exists\n",
    "import os\n",
    "expected_files = [\n",
    "    '/content/prosopo/prosopo/__init__.py',\n",
    "    '/content/prosopo/prosopo/models/arcface.py',\n",
    "    '/content/prosopo/prosopo/training/trainer.py',\n",
    "    '/content/prosopo/scripts/preprocess.py',\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for f in expected_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f'‚úÖ {f}')\n",
    "    else:\n",
    "        print(f'‚ùå MISSING: {f}')\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print('\\n‚úÖ Prosopo repo verified!')\n",
    "else:\n",
    "    print('\\n‚ùå Some files missing - check GitHub repo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Kaggle API\n",
    "\n",
    "‚ö†Ô∏è **DO NOT hardcode your API key!** Use Colab Secrets or upload kaggle.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload kaggle.json (RECOMMENDED)\n",
    "# Go to kaggle.com -> Settings -> API -> Create New Token\n",
    "# This downloads kaggle.json\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Check if already configured\n",
    "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "    print('Upload your kaggle.json file:')\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print('‚úÖ Kaggle configured')\n",
    "else:\n",
    "    print('‚úÖ Kaggle already configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Use Colab Secrets (Alternative)\n",
    "# Uncomment if you stored your key in Colab's secret manager\n",
    "\n",
    "# from google.colab import userdata\n",
    "# import os\n",
    "# \n",
    "# os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "# kaggle_json = f'''{{\"username\":\"{userdata.get('KAGGLE_USERNAME')}\",\"key\":\"{userdata.get('KAGGLE_KEY')}\"}}'''\n",
    "# with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "#     f.write(kaggle_json)\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# print('‚úÖ Kaggle configured from secrets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download CASIA-WebFace from Kaggle\n",
    "\n",
    "Dataset: `debarghamitraroy/casia-webface` (~2.73 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CASIA-WebFace dataset\n",
    "!kaggle datasets download -d debarghamitraroy/casia-webface -p /content/data/\n",
    "\n",
    "print('\\n‚úÖ Download complete. Checking contents...')\n",
    "!ls -lh /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and verify structure\n",
    "!unzip -q /content/data/casia-webface.zip -d /content/data/raw_rec\n",
    "\n",
    "print('\\nContents after unzip:')\n",
    "!find /content/data/raw_rec -name \"*.rec\" -o -name \"*.idx\" | head -20\n",
    "\n",
    "# Find the .rec file path\n",
    "import glob\n",
    "rec_files = glob.glob('/content/data/raw_rec/**/*.rec', recursive=True)\n",
    "print(f'\\nFound .rec files: {rec_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unpack RecordIO to Raw Images\n",
    "\n",
    "The dataset comes in MXNet RecordIO format. We unpack it to folders.\n",
    "\n",
    "‚è±Ô∏è **Time:** ~30 minutes for 490K images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import recordio\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unpack_rec_file(rec_path, output_dir):\n",
    "    \"\"\"\n",
    "    Unpack MXNet RecordIO file to image folders.\n",
    "    \"\"\"\n",
    "    print(f\"Unpacking {rec_path}...\")\n",
    "    \n",
    "    idx_path = rec_path.replace('.rec', '.idx')\n",
    "    if not os.path.exists(rec_path):\n",
    "        raise FileNotFoundError(f\"{rec_path} not found!\")\n",
    "    if not os.path.exists(idx_path):\n",
    "        raise FileNotFoundError(f\"{idx_path} not found! (Required alongside .rec)\")\n",
    "    \n",
    "    # Open RecordIO\n",
    "    imgrec = recordio.MXIndexedRecordIO(idx_path, rec_path, 'r')\n",
    "    \n",
    "    # Read header\n",
    "    s = imgrec.read_idx(0)\n",
    "    header, _ = recordio.unpack(s)\n",
    "    \n",
    "    if isinstance(header.label, float):\n",
    "        num_images = int(header.label)\n",
    "    else:\n",
    "        num_images = int(header.label[0])\n",
    "    \n",
    "    print(f\"Total images: {num_images:,}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx in tqdm(range(1, num_images + 1), desc=\"Unpacking\", mininterval=1.0):\n",
    "        try:\n",
    "            s = imgrec.read_idx(idx)\n",
    "            header, img_data = recordio.unpack(s)\n",
    "            \n",
    "            img = mx.image.imdecode(img_data).asnumpy()\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            if isinstance(header.label, float):\n",
    "                label = int(header.label)\n",
    "            else:\n",
    "                label = int(header.label[0])\n",
    "            \n",
    "            folder_name = f\"{label:07d}\"\n",
    "            save_dir = os.path.join(output_dir, folder_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"{idx}.jpg\"\n",
    "            cv2.imwrite(os.path.join(save_dir, filename), img)\n",
    "            success_count += 1\n",
    "            \n",
    "        except Exception:\n",
    "            error_count += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Unpacked {success_count:,} images ({error_count} errors)\")\n",
    "    print(f\"   Output: {output_dir}\")\n",
    "    print(f\"   Identities: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and unpack the .rec file\n",
    "import glob\n",
    "\n",
    "rec_files = glob.glob('/content/data/raw_rec/**/*.rec', recursive=True)\n",
    "\n",
    "if not rec_files:\n",
    "    print(\"‚ùå No .rec file found! Checking directory structure...\")\n",
    "    !find /content/data -type f | head -30\n",
    "else:\n",
    "    rec_path = rec_files[0]\n",
    "    print(f\"Using: {rec_path}\")\n",
    "    unpack_rec_file(rec_path, '/content/data/raw_casia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Align Faces with MTCNN\n",
    "\n",
    "Detect faces and warp to canonical 112√ó112 pose.\n",
    "\n",
    "‚è±Ô∏è **Time:** ~2-4 hours for 490K images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run alignment\n",
    "!python /content/prosopo/scripts/preprocess.py \\\n",
    "    --input /content/data/raw_casia \\\n",
    "    --output /content/data/aligned_casia \\\n",
    "    --skip-existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify alignment results\n",
    "import os\n",
    "\n",
    "aligned_dir = '/content/data/aligned_casia'\n",
    "\n",
    "if os.path.exists(aligned_dir):\n",
    "    num_identities = len([d for d in os.listdir(aligned_dir) \n",
    "                          if os.path.isdir(os.path.join(aligned_dir, d))])\n",
    "    \n",
    "    total_images = sum(len(files) for _, _, files in os.walk(aligned_dir))\n",
    "    \n",
    "    print(f\"‚úÖ Alignment complete!\")\n",
    "    print(f\"   Identities: {num_identities:,}\")\n",
    "    print(f\"   Total aligned images: {total_images:,}\")\n",
    "    \n",
    "    # Check for class_indices.json\n",
    "    if os.path.exists(f\"{aligned_dir}/class_indices.json\"):\n",
    "        print(f\"   ‚úÖ class_indices.json exists\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è class_indices.json not found\")\n",
    "else:\n",
    "    print(\"‚ùå Aligned directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download LFW for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LFW\n",
    "!wget -q http://vis-www.cs.umass.edu/lfw/lfw.tgz -O /content/data/lfw.tgz\n",
    "!tar -xzf /content/data/lfw.tgz -C /content/data/\n",
    "!wget -q http://vis-www.cs.umass.edu/lfw/pairs.txt -O /content/data/pairs.txt\n",
    "\n",
    "print('‚úÖ LFW downloaded')\n",
    "!ls /content/data/lfw | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/prosopo')\n",
    "\n",
    "# Test import\n",
    "try:\n",
    "    from prosopo.training import TrainingConfig, Trainer\n",
    "    from prosopo.models import Prosopo\n",
    "    print('‚úÖ Prosopo imports successful')\n",
    "except ImportError as e:\n",
    "    print(f'‚ùå Import failed: {e}')\n",
    "    print('\\nCheck that all files were pushed to GitHub!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prosopo.training import TrainingConfig\n",
    "\n",
    "config = TrainingConfig(\n",
    "    # Data paths\n",
    "    data_root='/content/data/aligned_casia',\n",
    "    class_indices_path='/content/data/aligned_casia/class_indices.json',\n",
    "    lfw_root='/content/data/lfw',\n",
    "    lfw_pairs_path='/content/data/pairs.txt',\n",
    "    \n",
    "    # Model\n",
    "    backbone='resnet50',\n",
    "    embedding_dim=512,\n",
    "    pretrained=True,\n",
    "    \n",
    "    # ArcFace\n",
    "    arcface_scale=64.0,\n",
    "    arcface_margin=0.5,\n",
    "    \n",
    "    # Training\n",
    "    batch_size=128,\n",
    "    accumulation_steps=2,\n",
    "    epochs=25,\n",
    "    lr=0.1,\n",
    "    num_workers=2,\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir='/content/drive/MyDrive/prosopo/checkpoints',\n",
    "    save_every=1,\n",
    "    val_epochs=[10, 15, 20, 25],\n",
    "    \n",
    "    # Resume (set path if session crashed)\n",
    "    resume_from=None,  # e.g., '/content/drive/MyDrive/prosopo/checkpoints/epoch_10.pth'\n",
    ")\n",
    "\n",
    "print('‚úÖ Config ready')\n",
    "print(f'   Batch size: {config.batch_size} x {config.accumulation_steps} = {config.batch_size * config.accumulation_steps} effective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "‚è±Ô∏è **Expected time:** ~8-12 hours on T4 GPU\n",
    "\n",
    "If session disconnects:\n",
    "1. Re-run cells 1-6 (they're fast - data is cached)\n",
    "2. Set `resume_from` to last checkpoint path\n",
    "3. Re-run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prosopo.training import Trainer\n",
    "\n",
    "trainer = Trainer(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prosopo.evaluation import evaluate_lfw\n",
    "\n",
    "accuracy, threshold = evaluate_lfw(\n",
    "    trainer.model,\n",
    "    config.lfw_root,\n",
    "    config.lfw_pairs_path,\n",
    ")\n",
    "\n",
    "print(f'\\nüéØ LFW Accuracy: {accuracy:.2%}')\n",
    "print(f'   Optimal threshold: {threshold:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "final_path = '/content/drive/MyDrive/prosopo/prosopo_final.pth'\n",
    "torch.save(trainer.model.state_dict(), final_path)\n",
    "print(f'‚úÖ Model saved to: {final_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(final_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
